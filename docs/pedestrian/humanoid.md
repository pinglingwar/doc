# [人形机器人仿真](https://github.com/google-deepmind/mujoco/network/dependents)

<!-- 更新到最新的仓库： https://github.com/murataliev/reinforcement_learning 
更新到的页面： https://github.com/google-deepmind/mujoco/network/dependents?dependents_before=MjUwOTM2Nzc0MDA -->

## Mujoco

[官方的移动任务实现](https://github.com/google-deepmind/dm_control/tree/main/dm_control/locomotion)

[层次世界模型实现人形全身控制](https://github.com/nicklashansen/puppeteer)

[模仿学习基准专注于使用 MuJoCo 执行复杂的运动任务](https://github.com/robfiras/loco-mujoco)

[全身控制的层次世界模型](https://github.com/nicklashansen/puppeteer)

[MyoSuite](https://github.com/MyoHub/myosuite) - 使用 MuJoCo 物理引擎模拟的肌肉骨骼模型要解决的环境/任务的集合，并包含在 OpenAI gym API 中

[将机器人送入家庭并收集数据](https://github.com/AlexanderKhazatsky/household_robots)

[dm_robotics：为机器人研究创建和使用的库、工具和任务](https://github.com/google-deepmind/dm_robotics)

### 建模

[OpenSim 肌肉骨骼模型转](https://github.com/MyoHub/myoconverter)

[Menagerie：MuJoCo物理引擎高质量模型集合](https://github.com/google-deepmind/mujoco_menagerie)

### 比赛

[足球射门、乒乓球对打](https://sites.google.com/view/myosuite/myochallenge/myochallenge-2025) 


## ROS

[乐聚机器人控制](https://github.com/LejuRobotics/kuavo-ros-opensource) - 包含 Mujoco 仿真环境

[将 ROS 与 MuJoCo 结合使用的封装器、工具和附加 API](https://github.com/ubi-agni/mujoco_ros_pkgs) - 支持 Noetic，- 其他 [mujoco_ros2_control](https://github.com/moveit/mujoco_ros2_control) 

[车道跟随器与强化学习](https://github.com/3N4N/ros-rl)

[一款一体化 ROS 软件包 RoTools](https://github.com/DrawZeroPoint/RoTools) - 用于高级机器人任务调度、视觉感知、路径规划、仿真以及直接/远程操控。它利用 BehaviorTree 实现快速的任务构建和协调，并提供各种实用程序来弥合真实/模拟机器人与高级任务调度程序之间的差距。

## 人

[使用 MuJoCo 物理引擎模拟的肌肉骨骼模型要解决的环境](https://github.com/MyoHub/myosuite) - 包含在 OpenAI gym API 中

[将 opensim 4.0+ MSK 模型转换为 MuJoCo 格式的工具](https://github.com/MyoHub/myoconverter) - 具有优化的肌肉运动学和动力学

[在MuJoCo中建模和模拟人机交互任务](https://github.com/User-in-the-Box/user-in-the-box) - 用户采用具有感知能力（例如自我中心视觉）的肌肉驱动生物力学模型进行建模，并通过强化学习进行训练以解决交互任务

[sin-cassie-rl-python](https://github.com/cwjwudi/sin-cassie-rl-python)

[仿生机器人](https://github.com/MHaqui/Biomecanical_project)

[构建意识与决策机制](https://github.com/oyako-li/TPG)

[用于机器人、深度强化学习和神经科学研究的 Python 工具](https://github.com/shandilya1998/neurorobotics)



## 强化学习

[使用 OpenAI Gym 环境的 xArm6 机器人强化学习框架](https://github.com/julio-design/xArm6-Gym-Env) - 该模型使用深度确定性策略梯度(DDPG) 进行连续动作，并使用后见之明经验回放(HER)

[四足动物-斯坦福小狗文档和训练学习者](https://github.com/Prakyathkantharaju/Quadruped)

[强化学习算法的最小实现及其他强化学习相关实验](https://github.com/PierreLaur/rl_experiments)

[基于深度 Q 网络的 TensorFlow 2 强化学习实现](https://github.com/metr0jw/DeepRL-TF2-DQN-implementation-for-TensorFlow-2)

[实现的主要算法是 Soft Actor-Critic (SAC)](https://github.com/tomaskak/neural)

[使用 OpenAI gym 环境训练 DQN 的简单脚本](https://github.com/eyalhagai12/simple_dqn)

[RL 控制和预测方法的实现（PyTorch 中的 DRL）](https://github.com/MythraV/rl_lib)

[基于技能的基于模型的强化学习](https://github.com/clvrai/skimo)

[RNN + PPO pytorch 实现](https://github.com/Amaranth819/RecurrentPPO)

[gym上强化学习的一些实现](https://github.com/paoyw/RL_gym)

[基于运动原语的 RL 算法的测试设置](https://github.com/freiberg-roman/mp-rl)

[使用 Openai-gym 进行强化学习](https://github.com/ashiskb/RL-workspace)

[基于因果模型的强化学习工具包](https://github.com/polixir/causal-mbrl)

[从头开始实现 rl2](https://github.com/neverparadise/RL2_from_scratch)

[不同 RL 和轨迹优化算法的实现](https://github.com/Daniellayeghi/Mujoco_Python_Sandbox)

[在 OpenAI Gym 环境中为超级马里奥兄弟实现强化学习](https://github.com/sampadk04/openai-super-mario-bros) - 使用近端策略优化 (PPO) 算法

[基于模型的连续强化学习中的随机值梯度](https://github.com/facebookresearch/svg)

[强化学习](https://github.com/murataliev/reinforcement_learning)

[用于样本有效目标条件强化学习的度量残差网络](https://github.com/Cranial-XIX/metric-residual-network)

### 多智能体

[个别奖励扶助的多智能体强化学习](https://github.com/MDrW/ICML2022-IRAT)

### 模仿学习

[通过语境翻译进行观察模仿](https://github.com/medric49/imitation-from-observation) - 一种基于演示训练代理模仿专家的算法

[通过行为学习进行观察模仿](https://github.com/medric49/ifobl)

### 分层强化学习

[使用 Pytorch、OpenAI Gym 和 Mujoco 进行机器人分层强化学习](https://github.com/mrernst/hrl_robotics_research)

[测试稳定比例微分控制器中 mujoco 的 SPD 实现](https://github.com/rohit-kumar-j/SPD_Controller_Mujoco)

## 控制

[一种基于视觉模型的强化算法 Dreamer](https://github.com/adityabingi/Dreamer) - 它学习一个世界模型，该模型从高级像素图像中捕捉潜在动态，并完全在从学习到的世界模型中想象的部署中训练控制代理

[基于对比示例的控制](https://github.com/khatch31/laeo)

[探索关节空间中潜在地标](https://github.com/dtch1997/latent-landmarks)

[倒立摆](https://github.com/dhruvthanki/mj_InvertedPendulum) - 使用基于优化的操作空间控制来跟踪双摆的末端执行器位置


## 测试

[评估了 RL 领域的特征提取](https://github.com/clement-chupin/BenchNeuralNework)

[l2r 基准测试](https://github.com/arav-agarwal2/l2r-benchmarks)

[Mujoco测试平台](https://github.com/AIRLABkhu/MujocoTestbed)

[针对机器人操作的基准测试集](https://github.com/xiangyanfei212/RMBench-2022)

## 数据

[用于模拟人形控制的多任务数据集](https://github.com/microsoft/MoCapAct)

[可变形物体操控沙盒](https://github.com/nicholasprayogo/dom_sandbox)

## 研究

[谷歌研究库](https://github.com/google-research/google-research)

[信念状态编码器/解码器](https://github.com/lucidrains/anymal-belief-state-encoder-decoder-pytorch) - 似乎产生了一种可与波士顿动力手工算法（四足动物 Spot）相媲美的策略

[SIMCSUM](https://github.com/timkolber/mtl_sum)

[NaturalNets](https://github.com/neuroevolution-ai/NaturalNets)

[去噪 MDP：比世界本身更好地学习世界模型](https://github.com/facebookresearch/denoised_mdp)

[深度强化学习中的首因偏差](https://github.com/evgenii-nikishin/rl_with_resets) - 深度强化学习代理的 JAX 实现，带有重置功能

[利用扩散模型作为高表达性的策略类别](https://github.com/twitter/diffusion-rl) - 用于行为克隆和策略正则化

[构建目标驱动的具身化大脑模型](https://github.com/ccnmaastricht/angorapy)

[稳定神经近似的逆向经验重放](https://github.com/google-research/look-back-when-surprised)

## 学习

[采样策略梯度扩展](https://github.com/BharathRajM/Sampled-Policy-Gradient-and-variants)

[cs285](https://github.com/johnviljoen/cs285)

[CS 285 作业](https://github.com/LeslieTrue/cs285_fall22_hw_sol)

[高级机器学习（AML）课程项目启动代码](https://github.com/gabrieletiboni/aml22-rl)

[数据分析与人工智能课程考试项目起始代码](https://github.com/gabrieletiboni/daai22-rl)

[毕业论文](https://github.com/Balssh/Thesis)

[CS285 的最终项目代码库：加州大学伯克利分校的深度强化学习](https://github.com/chirag-sharma-00/CS285-Project)

[CS285-Research-Project](https://github.com/aaggarw99/CS285-Research-Project)

[HPC_3](https://github.com/LBatov/HPC_3)

[使用 KNN 算法根据观察结果预测动作](https://github.com/abhayrcodes/cs285knn)

[一个利用强化学习、线性代数和机器人技术概念的实践项目](https://github.com/virajbshah/rl-inator)

[2022/2023 自主代理课程练习](https://github.com/lucalazzaroni/Autonomous-Agents)

[CS 285 家庭作业：深度强化学习](https://github.com/reecehuff/CS285_Homeworks)

[CIFAR-10-练习](https://github.com/RETELLIGENCE-IWEN/CIFAR-10-Practice)

[CS285 - 深度强化学习资料](https://github.com/Naghipourfar/cs285)

[伯克利 CS 285的作业：深度强化学习、决策和控制](https://github.com/GeoffNN/c285-HW)

[策略梯度](https://github.com/bvanbuskirk/PolicyGradient/tree/main/hw2)

[ELEC-E812课程作业](https://github.com/johnson-li/ELEC-E8125)

[用于 CS 391R 课程项目的击球机器人](https://github.com/jbonyun/cs391r)

[ÚFAL 课程 NPFL122](https://github.com/ufal/npfl122)

[伯克利 CS 285的作业：深度强化学习、决策和控制](https://github.com/berkeleydeeprlcourse/homework_fall2022)

## 任务

[基于物理的乒乓球](https://github.com/AayushSabharwal/physics-pong)

[四轴飞行器有效载荷抓取与运输轨迹规划与控制设计](https://github.com/antalpeter1/tdk-2022)

[使用 RL 和低级控制器控制四轴飞行器](https://github.com/Prakyathkantharaju/quadcopter)

## 平台

[Gym](https://github.com/openai/gym) - 用于开发和比较强化学习算法，它提供了一个用于在学习算法和环境之间进行通信的标准 API，以及一组兼容该 API 的标准环境。已迁移至 [Gymnasium](https://github.com/Farama-Foundation/Gymnasium) 

[Agility Robotics 的 Cassie 机器人的 mujoco 模拟因尽可能快地向前行走/奔跑而获得奖励](https://github.com/perrin-isir/gym-cassie-run) - [其他](https://github.com/AlexandreChenu/gym_cassie_dcil)

[通才generalist](https://github.com/grahamannett/generalist)


## 工具

[一个基于 C++ 的批处理环境池 EnvPool](https://github.com/sail-sg/envpool) - 基于 C++ 的高性能并行环境执行引擎（矢量化环境），适用于通用 RL 环境

[用于强化学习的机器人模拟环境集合](https://github.com/Farama-Foundation/Gymnasium-Robotics)

[提供了一个用于在学习算法和环境之间进行通信的标准 API](https://github.com/fliegla/diffDrive)

[包含 Google Research发布的代码](https://github.com/Rulial/GoogleRe-Pi)

[Emei 是一个用于开发因果强化学习算法的工具包](https://github.com/polixir/emei)

[JAX（Flax）实现具有连续动作空间的深度强化学习算法](https://github.com/ikostrikov/jaxrl)

[用于处理 MuJoCo 中使用的复合 Wavefront OBJ 文件的 CLI](https://github.com/kevinzakka/obj2mjcf)

[用于执行无梯度优化的 Python 工具箱](https://github.com/facebookresearch/nevergrad)



## 杂项

[InterGP](https://github.com/tdardinier/InterGP) - 收集数据、训练代理的流程

[ACM AI 所有研讨会内容代码等的存储库](https://github.com/acmucsd/acm-ai-workshops) - 内容按季度组织

[Docker Wiki 和示例](https://github.com/dotd/docker_wiki)

[强化学习实验](https://github.com/parthh01/rl_stuff)

[模块化部署](https://github.com/olivier-serris/ModularRollouts)

[玩具 ML 项目](https://github.com/pauldb89/ml)

[ML/DL/CS 领域的一些工作清单](https://github.com/vshmyhlo/research) - 包括基于 GAN 的图像生成、物体检测、神经机器翻译、相似性和度量学习、语音转文本、文本转语音

[CHTC 上的 Mujoco](https://github.com/NicholasCorrado/CHTC)

[从各种来源尝试的实践课程](https://github.com/BhaskarJoshi-01/Competitive-Programming)

[这是Spinning Up的一个克隆版本，目标是使用最新的 PyTorch 版本](https://github.com/haha1227/spinningup-pytorch)

[Fast Campus 强化学习](https://github.com/Junyoungpark/ReinforcementLearningAtoZ)

[Reddit 评论机器人是一个基于 Python 的自动回复器](https://github.com/benparrysapps/comment-meme-generator)

[一些强化学习的算法](https://github.com/etu7912a48/RL_algorithm) - 使用的环境是Windows10上的Python 3.10

[Gym的欠驱动机器人](https://github.com/RonAvr/Underactuated_with_gym)